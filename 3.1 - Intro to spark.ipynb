{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Spark\n",
    "---\n",
    "Python meets cluster computing\n",
    "\n",
    "<img src='img/pyspark.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Qué es spark?\n",
    "---\n",
    "Apache Spark combina un sistema de computación distribuida a través de clusters de ordenadores con una manera sencilla y elegante de escribir programas. Fue creado en la Universidad de Berkeley en California y es considerado el primer software de código abierto que hace la programación distribuida realmente accesible a los científicos de datos.\n",
    "\n",
    "Spark mantiene la escalabilidad lineal y la tolerancia a fallos de MapReduce, pero amplía sus bondades gracias a varias funcionalidades: **DAG** y **RDD**\n",
    "\n",
    "**DAG**: Directed Acyclic Graph. Un grafo dirigido que no tiene ciclos, un vértice se conecta a otro, pero nunca a si mismo.\n",
    "\n",
    "<img src='img/dag.jpg' width=300> \n",
    "\n",
    "**RDD**: Resilient Distributed Dataset. Es la estructura de datos fundamental de Spark y es una colección inmutable y distribuída de objetos. Cada set de datos en RDD se divide en particiones lógicas que pueden ser computadas en diferentes nodos. Formalmente, un RDD es una colección de objetos particionada de sólo lectura. Existen dos formas de crear RDD's, o bien a partir de paralelizar una colección existente o referenciandonos a un set de datos en un sistema de almacenamiento externo como HDFS.\n",
    "\n",
    "<img src='img/rdd-process.png' width=500>\n",
    "\n",
    "\n",
    "Spark está basado y extiende en el modelo de MapReduce para usarlo de forma eficiente en otro tipo de calculos. Su principal característia es la computación distribuída en memoria, lo cual incrementa de forma significativa la velocidad de procesado.\n",
    "\n",
    "En comparación con MapReduce, el cual crea un DAG con dos estados predefinidos (Map y Reduce), los grafos DAG creados por Spark pueden tener cualquier número de etapas. **Spark** con DAG es más rápido que MapReduce por el hecho de que **no tiene que escribir en disco los resultados obtenidos en las etapas intermedias del grafo.** MapReduce, sin embargo, debe escribir en disco los resultados entre las etapas Map y Reduce, lo cual resta velocidad.\n",
    "\n",
    "[fuente](https://geekytheory.com/apache-spark-que-es-y-como-funciona)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lazy evaluation\n",
    "---\n",
    "Una vez que los datos han sido leídos como objetos RDD en Spark, pueden realizarse dos tipos de operaciones sobre estos:\n",
    "\n",
    "1. **Transformaciones:** tras aplicar una transformación, obtenemos un nuevo y modificado RDD basado en el original. Ejemplos típicos: `map`, `filter`, `flatMap`, `join`, `cogroup`, `groupByKey`, `reduceByKey`, `countByKey`, y `sample`\n",
    "\n",
    "1. **Acciones:** una acción consiste simplemente en aplicar una operación sobre un RDD y obtener un valor como resultado, que dependerá del tipo de operación. Ejemplos típicos: `reduce`, `count`, `first`, `take`, `takeSample`, `foreach`, `collect` (nos devuelven valores) y, `saveAsTextFile`, `saveAsSequenceFil`, etc que nos guardan en disco los cálculos.\n",
    "\n",
    "Estos dos tipos de operación responden al hecho de que Spark es vago y utiliza lo que se conoce como *lazy evaluation*. Es decir, si nosotros le pedimos a Spark que opere en un set de datos de forma que nos realice una **transformación**, simplemente apunta la tarea sin ejecutar ningún cálculo y así de forma sucesiva hasta que finalmente le pedimos una respuesta final, es decir, **una acción**.\n",
    "\n",
    "Spark busca limitar al máximo la cantidad de trabajo que tiene que realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Que relación hay entre Spark y Haddop?\n",
    "---\n",
    "1. Como reemplazo. Spark nos permite gestionar y correr nuestro cluster propio de Spark de forma independiente de Hadoop.\n",
    "\n",
    "2. Como complemento. Podemos ejecutar Spark encima de un cluster de Hadoop y hacer uso de **YARN** y **HDFS**. De esta forma, Spark solo reemplaza a la capa de MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Aplicaciones de Spark\n",
    "---\n",
    "<img src='img/sparksession.png' width=300>\n",
    "\n",
    "• Spark tiene un gestor de cluster que mantiene una relación de los procesos disponibles\n",
    "\n",
    "• El proceso `driver` es responsable de ejecutar nuestra aplicación a traves de los `ejecutores` para terminar la tarea.\n",
    "\n",
    "Una de las principales ventajas de Spark es que, a pesar de estar escrito en Scala, interacciona con distintos lenguajes de programación de forma sencilla. Esto es gracias a que nuestro `driver` habla distintos lenguajes (python, scala, R, SQL, ...) y traduce las instrucciones a los ejecutores.\n",
    "\n",
    "<img src='img/pyspark-dataflow.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SparkSession\n",
    "---\n",
    "Como hemos comentado, controlamos nuestra aplicación de Spark a traves de un proceso `driver`. Este proceso se manifiesta al usuario como una sesión de Spark, que combina diferentes funcionalidades para manipular las abstracciones de datos de Spark (RDD's, Datasets, Dataframes).\n",
    "\n",
    "Básicamente es una combinación de SQLContext, HiveContext que de forma interna contienen un SparkContext para realizar los cálculos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SQL y DataFrames\n",
    "---\n",
    "Spark tiene distintas niveles de abstracción a la hora de estructurar los datos: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs).\n",
    "\n",
    "Todas estas abstracciones representan colecciones distribuidas de datos, aunque con diferentes interfaces para trabajar con ellos.\n",
    "\n",
    "## 6.1 SQL\n",
    "\n",
    "Spark SQL es un modulo de Spark para procesado de datos estructurados. A diferencia de los RDD, la interfaz de SQL provee a Spark con más información sobre la estructura de los datos (esquema) y el cálculo llevado a cabo. De forma interna, Spark usa esa información para aumentar el rendimiento de los cálculos.\n",
    "\n",
    "Spark SQL nos permite ejecutar queries de SQL sobre datos distribuidos y nos devolverá la respuesta en forma de DataFrame.\n",
    "\n",
    "## 6.2 Dataframes\n",
    "\n",
    "Un Dataset es una colección de datos distribuídos que nos proveen de los beneficios de los RDDs (usar funciones lambda potentes) con la optimización extra de Spark SQL.\n",
    "\n",
    "Finalmente, un Dataframe es un Dataset organizado en columnas con nombre. Es el equivalente a una tabla en una base de datos relacional o a un dataframe de `python` o `R`, pero muy optimizados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Material complementario con más detalles sobre funcionamiento y ejemplos **\n",
    "\n",
    "Data Scientists Guide to Apache Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
